项目技术难题与解决方案 
在本项目开发过程中，我遇到了涉及 Linux 驱动匹配、高并发 Socket 编程 以及 多线程竞态的深层次难题。以下是核心问题的复盘与解决过程。

1. I2C驱动与设备树匹配失败
[现象描述]
在设备树（Device Tree）中正确配置了 compatible 属性，且内核日志显示设备节点已创建，但驱动程序中的 probe 函数始终不执行。

[原因分析]
Linux内核的I2C子系统在匹配时存在“优先级和回退机制”。虽然现代内核优先匹配of_match_table，但某些特定版本或配置下的内核框架仍会检查旧的i2c_device_id列表。如果该列表缺失，或者列表中的字符串与设备树不严格一致，匹配就会失败。

[解决方案]
在驱动中同时保留 of_device_id 和 i2c_device_id 两种匹配表，并确保 ID 字符串完全一致。

2. Epoll 惊群与重复派发
[现象描述]
服务器在压力测试下，同一个cfd（客户端句柄）的事件会触发多次，导致多个工作线程同时抢夺同一个Socket进行读取。这造成了数据解析乱码和 EAGAIN 报错。

[排查过程]
尝试 LT 模式：发现 accept 后的 cfd 如果处理不及时，会导致所有空闲线程都在读取同一个fd。

尝试 ET 模式：虽然减少了触发次数，但在高并发瞬间，依然存在线程 A 未读完、线程 B 被唤醒的竞态。

[最终方案：EPOLLONESHOT]
在 epoll_ctl 注册事件时加入 EPOLLONESHOT 标志。

逻辑： 某个 Socket 的事件被触发后，内核会禁止该 Socket 再次产生事件，直到处理完并手动重置。

关键点： 线程处理完 6 字节数据发送后，需调用 epoll_ctl(..., MOD, ...) 重新使能监控。

1. 多线程任务参数的“内存覆盖”
[现象描述]
服务端反馈任务逻辑异常：客户端 A 收到了客户端 B 的数据，甚至出现了 lfd 的读函数误用了 cfd 变量的致命错误。

[根本原因]
最初在主循环中重复使用同一个局部变量 info 的地址传递给线程池。

竞态： 线程池里的工作线程还没来得及拷贝 info 中的值，主线程已经 accept 了下一个连接并修改了 info 内存块。

[优化设计]
改用 “堆区动态分配” 策略。

主循环： 每次 accept 成功后，malloc 一个独立的 task_info 结构体。

工作线程： 进入函数后先解引用，处理完业务后负责 free 该内存。

4. 监听套接字 (lfd) 的调度策略优化
【性能优化】
初期尝试： 将 lfd 的连接任务也放入线程池。

结果： 线程池在处理大量传感器数据（计算密集/IO密集）时，导致主线程分发的 accept 任务排队，客户端出现大量连接超时。

最终架构： 采用 “Reactor 模式思想”。

主循环（专职）： 只负责 epoll_wait 监听和 accept 连接。

线程池（专注）： 只负责处理已连接 Socket 的数据采集和发送。

效果： 系统吞吐量提升显著，不再出现连接阻塞。